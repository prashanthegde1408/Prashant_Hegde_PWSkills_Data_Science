{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2313cb57-e0e5-4824-a4d8-2b5cf031a973",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307405a8-8544-4348-b488-5b28f9319b2b",
   "metadata": {},
   "source": [
    "Web Scraping can be defined as a process of extracting content and data from the Internet. It provides automated methods to quickly scrape large amounts of data from websites or the Internet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1793a1-f4a0-407d-bb3c-21bbe46eb5f2",
   "metadata": {},
   "source": [
    "The amount of data we are generating is growing at an exponential rate. While this data has many sources, its most extensive repositories are Websites and the Internet. The method to extract this data from websites is called Web Scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4beda2-702b-4c8f-8ed7-f79d36bf61c5",
   "metadata": {},
   "source": [
    "Web Scraping has countless applications across industries. A few of the most common use cases of Web Scraping include -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54a58b-049e-4d08-aa00-c44852a2fe4e",
   "metadata": {},
   "source": [
    "Price Monitoring:\n",
    "\n",
    "Organizations scrape the pricing and other related information for their and competitors' products to analyze and fix optimal pricing for the products to maximize revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70033dbf-93b8-4d77-b533-08746eee16ad",
   "metadata": {},
   "source": [
    "Market Research:\n",
    "\n",
    "Organizations use Web Scraping to extract product data, reviews, and other relevant information to perform sentiment analysis, consumer trends, and competitor analysis.\n",
    "\n",
    "\n",
    "News Monitoring:\n",
    "\n",
    "Organizations dependent on daily news for their day-to-day functioning can use Web Scraping to generate reports based on the daily news.\n",
    "\n",
    "\n",
    "Sentiment Analysis:\n",
    "\n",
    "Companies can collect product-related data from Social Media such as Facebook, Twitter, etc., and other online forums to analyze the general sentiment for their products among consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c93f06-c754-4efe-bc51-d57e8aba1f95",
   "metadata": {},
   "source": [
    "##########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b0d470-ee74-4cca-a6fe-12fc97776f20",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717e117-478d-453f-b036-d4ed3a43dded",
   "metadata": {},
   "source": [
    "Web Scrapers can be categorized based on various factors, including Self-built or Pre-built Web Scraper, Browser Extension or Software Web Scrapers, and Local or Cloud Web Scrapers.\n",
    "\n",
    "Self-Built Web Scrapers require advanced programming knowledge to build and develop. If you want more features, it will require a more advanced understanding of programming languages. Pre-Built Web Scrapers can be downloaded and used directly. Some Scrapers also provide options to customize their features based on your requirements.\n",
    "\n",
    "Browser Extension Web Scrapers can be added to your browsers. These Scrapers generally have a limited set of features as these are dependent on compatibility with your browser. Software Web Scrapers does not suffer from these limitations as these can be downloaded and installed directly on your computer.\n",
    "\n",
    "Cloud Web Scrapers run on the cloud and will not use your local computer resources such as RAM, CPU, etc. Local Web Scrapers run on your computer and will consume its resources to perform Web Scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9996ba2d-31e0-47f1-a8d6-ca244a67a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c8984-f731-4a4e-9fdb-d21eee33cfc3",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f9921b-45a9-4cf4-a2a1-db3e78a9f08e",
   "metadata": {},
   "source": [
    "Beautiful Soup is a library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6f23d-444c-49b3-a3a9-105d92e76e81",
   "metadata": {},
   "source": [
    "It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e10a3-f253-43c5-b741-d4006775f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f7f5a-7624-4bd3-a99d-2ffb21402060",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8316b4c-40ad-4770-b129-01b6953f6f4e",
   "metadata": {},
   "source": [
    "Flask is used to integrate our web scraping project with rest API. we created a web scrapper with Beauitful Soup. but to have the scrapper integrated with an application, We used Flask and integrated it with rest api, and created an application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f7fdc-f512-44ef-a5ab-15764e180603",
   "metadata": {},
   "source": [
    "We used Flask to make an API so that it will be able to call the html and the backend web scraper, and then perform the search operation in the front end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9788f-b09e-4591-ba90-4e318532f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f0893-3fb5-4f32-84ba-890ed2d89e96",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0f97f-635d-49b4-af64-e21879d12267",
   "metadata": {},
   "source": [
    "AWS Elastic Beanstalk\n",
    "\n",
    "AWS Elastic Beanstalk is an AWS managed service for web applications. Elastic beanstalk is a pre-configured EC2 server that can directly take up your application code and environment configurations and use it to automatically provision and deploy the required resources within AWS to run the web application. Unlike EC-2 which is Infrastructure as a service, Elastic beanstalk is a Platform As A service (PAAS) as it allows users to directly us a pre-configured server for their application. Of course you can deploy applications without ever having to use elastic beanstalk but that would mean having to choose the appropriate service from the vast array of services offered by  AWS, manually provisioning these AWS resources and stitching them up together to form a complete web-application. Elastic beanstalk abstracts the underlying configuration work and allows you as a user to focus on more pressing matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b06446-124f-46ab-a89d-3a6149ed301f",
   "metadata": {},
   "source": [
    "AWS CodePipeline\n",
    "\n",
    "AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production. AWS CodePipeline then builds, tests, and deploys your application according to the defined workflow every time there is a code change. You can integrate partner tools and your own custom tools into any stage of the release process to form an end-to-end continuous delivery solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc1847-1efb-473c-ae98-26b1d3700ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
